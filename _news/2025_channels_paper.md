---
layout: post
title: "Flat Channels to Infinity in Neural Loss Landscapes"
date: 2025-06-17
inline: true
related_posts: false
---

{% include labels/paper_label.html %} We found a peculiar structure in the loss landscape: slowly decreasing loss channels that converge to a minimum at âˆž-norm in parameter space. Our new paper describes how these channels emerge from saddle points that are induced by permutation symmetries. Functionally, these minima at infinity implement Gated Linear Units via combinations of standard artificial neurons. Check out our [Flat Channels to Infinity in Neural Loss Landscapes](https://arxiv.org/abs/2506.14951).