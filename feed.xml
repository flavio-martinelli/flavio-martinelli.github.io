<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://flavio-martinelli.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://flavio-martinelli.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-04T11:56:30+00:00</updated><id>https://flavio-martinelli.github.io/feed.xml</id><title type="html">blank</title><subtitle>Webpage of Flavio Martinelli </subtitle><entry><title type="html">ReLU Playground: how complex are the dynamics of one neuron learning another one?</title><link href="https://flavio-martinelli.github.io/blog/2025/reluplayground/" rel="alternate" type="text/html" title="ReLU Playground: how complex are the dynamics of one neuron learning another one?"/><published>2025-04-23T00:00:00+00:00</published><updated>2025-04-23T00:00:00+00:00</updated><id>https://flavio-martinelli.github.io/blog/2025/reluplayground</id><content type="html" xml:base="https://flavio-martinelli.github.io/blog/2025/reluplayground/"><![CDATA[ <p>⚠️This page is under construction⚠️</p> <p>But in the meantime you can play with the relus by moving them around with the mouse, or type in some values, or move points in the phase-spaces. Press play to start learning!</p> <p>Can you find which initialisations can converge to the teacher? How do other solutions look like? By looking at the different phase spaces, can you get an intuitive understanding of the learning dynamics?</p> <link rel="stylesheet" href="/assets/js/reluplayground/style.css"/> <script src="/assets/js/reluplayground/drawUtils.js"></script> <script src="/assets/js/reluplayground/utils.js"></script> <script src="/assets/js/reluplayground/explanations.js"></script> <script src="/assets/js/reluplayground/option_inits.js"></script> <script src="/assets/js/reluplayground/interaction_input_boxes.js"></script> <script src="/assets/js/reluplayground/interactions_output_space.js"></script> <script src="/assets/js/reluplayground/buttons_and_visuals.js"></script> <script src="/assets/js/reluplayground/ml.js"></script> <script src="/assets/js/reluplayground/sketch.js"></script> <div class="container"> <div id="canvas-container"></div> </div> <p><strong>DETAILS:</strong></p> <ul> <li>The student network is parameterized with 4 variables: f(x) = a ReLU(wx + b) + c <li>We can transform the parameterization to make it phenomenologically intuitive, from 4 continuous variables to 3 continuous and 1 discrete. The kink of the relu is k=-b/w, while the slope is computed as m=a|w| and the direction of the relu is simply s=sign(w). c remains unchanged. The grey boxes show relu dynamics in different slices of the parameter space. <li>The teacher network is defined phenomenologically as t(x) = m relu(s(x-k)) + c. <li>The 1D input data is densely sampled with a uniform distribution (std=1). There are no datapoints outside the grey area of the "INPUT-OUTPUT SPACE". This creates some interesting and under-explored boundary effects. <li>Training is done through standard full-batch gradient descent. </li></li></li></li></li></ul>]]></content><author><name></name></author><category term="interactive"/><category term="learning"/><category term="dynamics"/><summary type="html"><![CDATA[An interactive playground ⛹️‍♂️]]></summary></entry></feed>