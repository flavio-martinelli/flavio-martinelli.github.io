---
---


@inproceedings{martinelli2020spiking,
  title={Spiking neural networks trained with backpropagation for low power neuromorphic implementation of voice activity detection},
  author={Martinelli, Flavio and Dellaferrera, Giorgia and Mainar, Pablo and Cernak, Milos},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8544--8548},
  year={2020},
  organization={IEEE},
  bibtex_show={true},
  abbr={ICASSP},
  selected={true},
  website={https://ieeexplore.ieee.org/abstract/document/9053412/},
  doi={10.1109/ICASSP40776.2020.9053412},
  code={https://codeocean.com/capsule/7317693/},
  preview={icassp2020.png},
  abstract={Recent advances in Voice Activity Detection (VAD) are driven by artificial and Recurrent Neural Networks (RNNs), however, using a VAD system in battery-operated devices requires further power efficiency. This can be achieved by neuromorphic hardware, which enables Spiking Neural Networks (SNNs) to perform inference at very low energy consumption. Spiking networks are characterized by their ability to process information efficiently, in a sparse cascade of binary events in time called spikes. However, a big performance gap separates artificial from spiking networks, mostly due to a lack of powerful SNN training algorithms. To overcome this problem we exploit an SNN model that can be recast into a recurrent network and trained with known deep learning techniques. We describe a training procedure that achieves low spiking activity and apply pruning algorithms to remove up to 85% of the network connections with no performance loss. The model competes with state-of-the-art performance at a fraction of the power consumption comparing to other methods.}
}


@inproceedings{dellaferrera2020bin,
  title={A bin encoding training of a spiking neural network based voice activity detection},
  author={Dellaferrera, Giorgia and Martinelli, Flavio and Cernak, Milos},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3207--3211},
  year={2020},
  organization={IEEE},
  bibtex_show={true},
  abbr={ICASSP},
  selected={false},
  website={https://ieeexplore.ieee.org/abstract/document/9054761},
  doi={10.1109/ICASSP40776.2020.9054761},
  code={https://codeocean.com/capsule/2406013/},
  preview={icassp2020b.png},
  abstract={Advances of deep learning for Artificial Neural Networks (ANNs) have led to significant improvements in the performance of digital signal processing systems implemented on digital chips. Although recent progress in low-power chips is remarkable, neuromorphic chips that run Spiking Neural Networks (SNNs) based applications offer an even lower power consumption, as a consequence of the ensuing sparse spikebased coding scheme. In this work, we develop a SNN-based Voice Activity Detection (VAD) system that belongs to the building blocks of any audio and speech processing system. We propose to use the bin encoding, a novel method to convert log mel filterbank bins of single-time frames into spike patterns. We integrate the proposed scheme in a bilayer spiking architecture which was evaluated on the QUT-NOISE-TIMIT corpus. Our approach shows that SNNs enable an ultra low-0power implementation of a VAD classifier that consumes only 3.8 μW, while achieving state-of-the-art performance.}
}


@article{brea2023mlpgradientflow,
  title={Mlpgradientflow: going with the flow of multilayer perceptrons (and finding minima fast and accurately)},
  author={Brea, Johanni and Martinelli, Flavio and {\c{S}}im{\c{s}}ek, Berfin and Gerstner, Wulfram},
  journal={arXiv preprint arXiv:2301.10638},
  year={2023},
  bibtex_show={true},
  abbr={ArXiv},
  selected={false},
  website={https://arxiv.org/abs/2301.10638},
  doi={https://doi.org/10.48550/arXiv.2301.10638},
  code={https://github.com/jbrea/MLPGradientFlow.jl},
  preview={MLPGradientFlow.png},
  abstract={MLPGradientFlow is a software package to solve numerically the gradient flow differential equation θ ̇ = −∇L(θ; D), where θ are the parameters of a multi-layer perceptron, D is some data set, and ∇L is the gradient of a loss function. We show numerically that adaptive first- or higher-order integration methods based on Runge-Kutta schemes have better accuracy and convergence speed than gradient descent with the Adam optimizer. However, we find Newton’s method and approximations like BFGS preferable to find fixed points (local and global minima of L) efficiently and accurately. For small networks and data sets, gradients are usually computed faster than in pytorch and Hessian are computed at least 5× faster. Additionally, the package features an integrator for a teacher-student setup with bias-free, two-layer networks trained with standard Gaussian input in the limit of infinite data. The code is accessible at https://github.com/jbrea/MLPGradientFlow.jl.}
}

@inproceedings{martinelli2023expand,
  title={Expand-and-Cluster: Parameter Recovery of Neural Networks},
  author={Martinelli, Flavio and Simsek, Berfin and Gerstner, Wulfram* and Brea, Johanni*},
  booktitle={Forty-first International Conference on Machine Learning (ICML 2024)},
  year={2024},
  bibtex_show={true},
  abbr={ICML},
  selected={true},
  website={https://openreview.net/pdf?id=3MIuPRJYwf},
  doi={},
  code={https://github.com/flavio-martinelli/expand-and-cluster},
  preview={EC.png},
  abstract={Can we identify the weights of a neural network by probing its input-output mapping? At first glance, this problem seems to have many solutions because of permutation, overparameterisation and activation function symmetries. Yet, we show that the incoming weight vector of each neuron is identifiable up to sign or scaling, depending on the activation function. Our novel method 'Expand-and-Cluster’ can identify layer sizes and weights of a target network for all commonly used activation functions. Expand-and-Cluster consists of two phases: (i) to relax the non-convex optimisation problem, we train multiple overparameterised student networks to best imitate the target function; (ii) to reverse engineer the target network's weights, we employ an ad-hoc clustering procedure that reveals the learnt weight vectors shared between students -- these correspond to the target weight vectors. We demonstrate successful weights and size recovery of trained shallow and deep networks with less than 10% overhead in the layer size and describe an 'ease-of-identifiability' axis by analysing 150 synthetic problems of variable difficulty.},
  annotation={* equal senior author contribution}
}
